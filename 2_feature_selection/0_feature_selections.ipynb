{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "'''\n",
    "feature selection\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** import package **\n",
    "import os\n",
    "import sys\n",
    "import json\n",
    "import pathlib\n",
    "sys.path.append(\"..\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "from _utils.preprocessing_xgboost import *\n",
    "from _utils.customlogger import customlogger as CL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** loading config **\n",
    "with open('./../{}'.format(\"config.json\")) as file:\n",
    "    cfg = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ** loading info **\n",
    "current_dir = pathlib.Path.cwd()\n",
    "parent_dir = current_dir.parent\n",
    "current_date = cfg[\"working_date\"]\n",
    "curr_file_name = os.path.splitext(os.path.basename(os.path.abspath('')))[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# **create Logger**\n",
    "log = CL(\"custom_logger\")\n",
    "pathlib.Path.mkdir(pathlib.Path('{}/_log/'.format(parent_dir)), mode=0o777, parents=True, exist_ok=True)\n",
    "log = log.create_logger(file_name=\"../_log/{}.log\".format(curr_file_name), mode=\"a\", level=\"DEBUG\")  \n",
    "log.debug('start {}'.format(curr_file_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersect(a, b):\n",
    "    return list(set(a) & set(b))\n",
    "\n",
    "def getPairedTTest(baseline_df, abnormal_df, concept_list):\n",
    "    baseline_df = baseline_df[baseline_df['label']==1]\n",
    "    abnormal_df = abnormal_df[abnormal_df['label']==1]\n",
    "    import scipy.stats\n",
    "    selected_var_df = pd.DataFrame()\n",
    "    concept_set = list(set(baseline_df.columns) & set(abnormal_df.columns) & set(concept_list))\n",
    "    # print(len(concept_set), concept_set)\n",
    "    for concept in concept_set:\n",
    "        # print(abnormal_df[concept].mean(), baseline_df[concept].mean())\n",
    "        statistic, pvalue = scipy.stats.ttest_ind(abnormal_df[concept], baseline_df[concept], equal_var=False, nan_policy='omit')\n",
    "        label_1_before = len(baseline_df[concept].dropna()) \n",
    "        label_1_after = len(abnormal_df[concept].dropna()) \n",
    "        label_1_before_mean = baseline_df[concept].dropna().mean() \n",
    "        label_1_after_mean = abnormal_df[concept].dropna().mean()\n",
    "        # print(concept, pvalue)\n",
    "        if statistic>1 and pvalue<0.05 :\n",
    "            # print(concept)\n",
    "            var_temp = {}\n",
    "            var_temp['concept_id'] = concept\n",
    "            var_temp['pvalue'] = pvalue\n",
    "            var_temp['label_1_before'] = label_1_before\n",
    "            var_temp['label_1_after'] = label_1_after\n",
    "            var_temp['label_1_before_mean'] = label_1_before_mean\n",
    "            var_temp['label_1_after_mean'] = label_1_after_mean\n",
    "            selected_var_df = selected_var_df.append(var_temp, ignore_index=True)\n",
    "    return selected_var_df\n",
    "\n",
    "def getMcnemarTest(baseline_df, abnormal_df, concept_list):\n",
    "    import scipy.stats\n",
    "    selected_var_df = pd.DataFrame()\n",
    "    concept_set = list(set(baseline_df.columns) & set(abnormal_df.columns) & set(concept_list))\n",
    "    # print(len(concept_set), concept_set)\n",
    "    for concept in concept_set:\n",
    "        label_0_before = len(baseline_df[(baseline_df['label']==0) & (baseline_df[concept]==1)])\n",
    "        label_1_before = len(baseline_df[(baseline_df['label']==1) & (baseline_df[concept]==1)])\n",
    "        label_0_after = len(abnormal_df[(abnormal_df['label']==0) & (abnormal_df[concept]==1)]) \n",
    "        label_1_after = len(abnormal_df[(abnormal_df['label']==1) & (abnormal_df[concept]==1)]) \n",
    "        arr_before = np.array([label_1_before, label_0_before])\n",
    "        arr_after = np.array([label_1_after, label_0_after])\n",
    "        table = np.vstack([arr_before, arr_after]) # vertical stack\n",
    "        table = np.transpose(table)             # trans pose\n",
    "        result = mcnemar(table, exact=True) # 샘플 수<25 일 경우 mcnemar(table, exact=False, correction=True)\n",
    "        if result.pvalue < 0.05 :\n",
    "            # print(concept)\n",
    "            var_temp = {}\n",
    "            var_temp['concept_id'] = concept\n",
    "            var_temp['pvalue'] = result.pvalue\n",
    "            var_temp['label_0_before'] = label_0_before\n",
    "            var_temp['label_0_after'] = label_0_after\n",
    "            var_temp['label_1_before'] = label_1_before\n",
    "            var_temp['label_1_after'] = label_1_after\n",
    "            selected_var_df = selected_var_df.append(var_temp, ignore_index=True)\n",
    "    return selected_var_df\n",
    "\n",
    "def average_duration_of_adverse_events(df):\n",
    "    df = df[['person_id', 'cohort_start_date', 'first_abnormal_date']].drop_duplicates() #.subject_id.unique()\n",
    "    df['c_f'] = df['first_abnormal_date'] - df['cohort_start_date']\n",
    "    # print(df['c_f'].describe())\n",
    "    return df['c_f'].mean().days\n",
    "\n",
    "def make_pivot(df):\n",
    "    if df.empty:\n",
    "        return pd.DataFrame()\n",
    "    print(\"person_id(count) : \", df.person_id.nunique(), \"concept_name(count) : \", df.concept_name.nunique())\n",
    "    df = df.sort_values(by=['person_id', 'concept_id', 'concept_date'], axis=0, ascending=True)\n",
    "    df['first_abnormal_date'] = pd.to_datetime(df['first_abnormal_date']).fillna(pd.to_datetime('1900-01-01'))\n",
    "    last_record_df = df.groupby(by=['person_id', 'concept_id']).apply(lambda x: x.iloc[-1]).reset_index(drop=True)\n",
    "    def subtract(x, y):\n",
    "        return [item for item in x if item not in set(y)]\n",
    "    pivot_cols = subtract(last_record_df.columns, ['concept_name', 'concept_date', 'concept_id', 'concept_value', 'concept_domain'])\n",
    "    pivot_df = pd.pivot_table(data = last_record_df, index = pivot_cols, columns='concept_id', values='concept_value').reset_index()\n",
    "    return pivot_df\n",
    "\n",
    "def impute_conditional_data(df, concept_ids):\n",
    "    cols = list(set(df.columns)&set(concept_ids))\n",
    "    df[cols] = df[cols].fillna(df[cols].median())\n",
    "    return df\n",
    "    \n",
    "def impute_binary_data(df, concept_ids):\n",
    "    cols = list(set(df.columns)&set(concept_ids))\n",
    "    df[cols] = df[cols].fillna(0)\n",
    "    return df\n",
    "\n",
    "def normalization_Robust(df):\n",
    "    from sklearn.preprocessing import RobustScaler\n",
    "    transformer = RobustScaler()\n",
    "    transformer.fit(df)\n",
    "    df = transformer.transform(df) \n",
    "    return df \n",
    "\n",
    "def normalization_std(df):\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    transformer = StandardScaler()\n",
    "    transformer.fit(df)\n",
    "    df = transformer.transform(df) \n",
    "    return df \n",
    "\n",
    "def normalization_minmax(df):\n",
    "    from sklearn.preprocessing import MinMaxScaler\n",
    "    transformer = MinMaxScaler()\n",
    "    transformer.fit(df)\n",
    "    df = transformer.transform(df) \n",
    "    return df \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_selection import chi2, SelectKBest, SelectFromModel, SelectPercentile, VarianceThreshold, mutual_info_classif\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "def add_column_concept_name(df, concept_id_name_dict):\n",
    "    df['concept_name'] = df.apply(lambda x: concept_id_name_dict[x.concept_id] if x.concept_id in concept_id_name_dict.keys() else x.concept_id, axis = 1)\n",
    "    return df\n",
    "\n",
    "def add_column_concept_domain(df, concept_id_domain_dict):\n",
    "    df['concept_domain'] = df.apply(lambda x: concept_id_domain_dict[x.concept_id] if x.concept_id in concept_id_domain_dict.keys() else 'common', axis = 1)\n",
    "    print(df.concept_domain.value_counts())\n",
    "    return df\n",
    "\n",
    "def make_concepts_df(selected_features, concept_id_name_dict, concept_id_domain_dict):\n",
    "    if len(selected_features) < 1:\n",
    "        return pd.DataFrame()\n",
    "        \n",
    "    df = pd.DataFrame(selected_features, columns =['concept_id'])\n",
    "    df = add_column_concept_name(df, concept_id_name_dict)\n",
    "    df = add_column_concept_domain(df, concept_id_domain_dict)\n",
    "    print(len(df.concept_id.unique()))\n",
    "    return df\n",
    "\n",
    "def write_file_method(df, dir, name, method):\n",
    "    if df.empty:\n",
    "        return False\n",
    "    full_file_path = pathlib.Path('{}/{}_{}.csv'.format(dir, name, method))\n",
    "    df.to_csv(full_file_path, index=False, float_format='%g')\n",
    "    return True\n",
    "\n",
    "def read_files_method(dir, name, method):\n",
    "    full_file_path = pathlib.Path('{}/{}_{}.csv'.format(dir, name, method))\n",
    "    if not pathlib.Path.exists(full_file_path):\n",
    "        return pd.DataFrame()\n",
    "    df = pd.read_csv(full_file_path)\n",
    "    return df\n",
    "\n",
    "def read_files_all_methods(dir, name):\n",
    "    methods = ['statistics', 'VT', 'KBest', 'percentile', 'ExtraTrees', 'lasso_0_1', 'lasso_0_0_1', 'mutual']\n",
    "    concat_df = pd.DataFrame()\n",
    "    for method in methods:\n",
    "        method_df = read_files_method(dir, name, method)\n",
    "        if method_df.empty:\n",
    "            continue\n",
    "        method_df['method'] = method\n",
    "        concat_df = pd.concat([concat_df, method_df], axis=0)\n",
    "    return concat_df\n",
    "\n",
    "def resumetable(df):\n",
    "    print(f'data frame shape: {df.shape}')\n",
    "    summary = pd.DataFrame(df.dtypes, columns=['data_type'])\n",
    "    summary = summary.reset_index()\n",
    "    summary = summary.rename(columns={'index': 'feature'})\n",
    "    summary['n_values'] = df.notnull().sum().values\n",
    "    summary['n_missingvalues'] = df.isnull().sum().values\n",
    "    summary['n_missingrate'] = df.isnull().sum().values/len(df)\n",
    "    summary['n_eigenvalues'] = df.nunique().values\n",
    "    return summary\n",
    "\n",
    "def getxydata(df):\n",
    "    x_df = df.drop(['person_id', 'cohort_start_date', 'first_abnormal_date', 'label'], axis=1) \n",
    "    # x_df = (x_df-x_df.min())/(x_df.max()-x_df.min()) # normalize\n",
    "    x_data = x_df.to_numpy()\n",
    "    y_data = df['label'].to_numpy()\n",
    "    cols = x_df.columns\n",
    "    return x_data, y_data, cols\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_all_methods(feature_selection_method_df_dict):\n",
    "    concat_df = pd.DataFrame()\n",
    "    for method in feature_selection_method_df_dict.keys():\n",
    "        method_df = feature_selection_method_df_dict[method]\n",
    "        if method_df.empty:\n",
    "            continue\n",
    "        method_df['method'] = method\n",
    "        concat_df = pd.concat([concat_df, method_df], axis=0)\n",
    "    return concat_df\n",
    "\n",
    "def merge_summary_table_df(summary_df, concat_df):\n",
    "    pivot_df = concat_df[['concept_id', 'concept_name', 'concept_domain', 'method']]\n",
    "    pivot_df['value'] = 1\n",
    "    pivot_df = pd.pivot_table(data=pivot_df, columns='method', index=['concept_id', 'concept_name', 'concept_domain'], values='value', fill_value=0).reset_index()\n",
    "    pivot_df['total'] = pivot_df[list(set(concat_df.method.unique()) & set(pivot_df.columns))].sum(axis=1)\n",
    "    pivot_df\n",
    "    pivot_df.concept_id = pivot_df.concept_id.apply(lambda _: str(_).replace('.0',''))\n",
    "    summary_df.concept_id = summary_df.concept_id.apply(lambda _: str(_).replace('.0',''))\n",
    "    pivot_join_df = pd.merge(left=pivot_df, right=summary_df, left_on=['concept_id'], right_on=['concept_id'], how='left')\n",
    "    # old_columns = pivot_join_df.columns.to_list()\n",
    "    # new_columns = ['{}_{}'.format(col,hospital[0]) for col in pivot_join_df.columns]\n",
    "    # pivot_join_df.rename(dict(zip(old_columns, new_columns)), axis=1, inplace=True)\n",
    "    return pivot_join_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[ ]:\n",
    "def runTask(outcome_name):\n",
    "    # outcome_name = 'Cisplatin'\n",
    "    log.debug(\"{}\".format(outcome_name))\n",
    "\n",
    "    importsql_output_dir    = pathlib.Path('{}/data/{}/importsql/{}/'.format(parent_dir, current_date, outcome_name))\n",
    "    output_data_dir         = pathlib.Path('{}/data/{}/feature_selection/{}/'.format(parent_dir, current_date, outcome_name))\n",
    "    output_result_dir       = pathlib.Path('{}/result/{}/feature_selection/{}/'.format(parent_dir, current_date, outcome_name))\n",
    "    pathlib.Path.mkdir(output_data_dir, mode=0o777, parents=True, exist_ok=True)\n",
    "    pathlib.Path.mkdir(output_result_dir, mode=0o777, parents=True, exist_ok=True)\n",
    "\n",
    "    # In[ ]: @load data\n",
    "    meas_df = pd.read_csv('{}/{}_meas_df.txt'.format(importsql_output_dir, outcome_name), low_memory=False)\n",
    "    drug_df = pd.read_csv('{}/{}_drug_df.txt'.format(importsql_output_dir, outcome_name), low_memory=False)\n",
    "    proc_df = pd.read_csv('{}/{}_proc_df.txt'.format(importsql_output_dir, outcome_name), low_memory=False)\n",
    "    cond_df = pd.read_csv('{}/{}_cond_df.txt'.format(importsql_output_dir, outcome_name), low_memory=False)\n",
    "\n",
    "    # In[ ]: @fill concept_value\n",
    "    drug_df['concept_value'] = 1 # temp code\n",
    "    proc_df['concept_value'] = 1\n",
    "    cond_df['concept_value'] = 1\n",
    "\n",
    "    # In[ ]: @use only necessary columns\n",
    "    common_cols = ['person_id', 'age', 'sex', 'cohort_start_date', 'first_abnormal_date', 'concept_date', 'concept_id', 'concept_name', 'concept_value', 'concept_domain', 'label']\n",
    "\n",
    "    meas_df = meas_df[common_cols]\n",
    "    drug_df = drug_df[common_cols]\n",
    "    proc_df = proc_df[common_cols]\n",
    "    cond_df = cond_df[common_cols]\n",
    "\n",
    "    log.info(\"[nData] m : {} d : {} p : {}  c : {} all : {}\".format(len(meas_df), len(drug_df), len(proc_df), len(cond_df), (len(meas_df) + len(drug_df) + len(proc_df) + len(cond_df))))\n",
    "\n",
    "    # In[ ]: @Remove feature used in outcome define\n",
    "    drug_name = outcome_name\n",
    "    drug_concept_ids_excluded = map(int,cfg['drug'][drug_name]['@drug_concept_set'].split(','))\n",
    "    drug_df = drug_df.loc[~drug_df.concept_id.isin(drug_concept_ids_excluded)]\n",
    "    meas_concept_ids_excluded = map(int,[cfg['meas'][meas_name]['@meas_concept_id'] for meas_name in cfg['meas']])\n",
    "    meas_df = meas_df.loc[~meas_df.concept_id.isin(meas_concept_ids_excluded)]\n",
    "\n",
    "    # In[ ]: @valid data processing for cohorts.\n",
    "    meas_df = cohortConditionSetting(meas_df, pre_observation_period=60, post_observation_peroid=60)\n",
    "    drug_df = cohortConditionSetting(drug_df, pre_observation_period=60, post_observation_peroid=60)\n",
    "    proc_df = cohortConditionSetting(proc_df, pre_observation_period=60, post_observation_peroid=60)\n",
    "    cond_df = cohortConditionSetting(cond_df, pre_observation_period=60, post_observation_peroid=60)\n",
    "\n",
    "    ndays = average_duration_of_adverse_events(cond_df)\n",
    "    log.debug('average_duration_of_adverse_events : {}'.format(ndays))\n",
    "\n",
    "    all_domain_df = pd.concat([meas_df, drug_df, proc_df, cond_df], axis=0, ignore_index=True)\n",
    "    all_domain_baseline_df = all_domain_df.query('cohort_start_date >= concept_date')\n",
    "\n",
    "    if all_domain_df.empty:\n",
    "        return\n",
    "\n",
    "    all_domain_pivot_df = make_pivot(all_domain_df)\n",
    "    all_domain_pivot_baseline_df = make_pivot(all_domain_baseline_df)\n",
    "\n",
    "    summary_df = resumetable(all_domain_pivot_df)\n",
    "    write_file_method(summary_df, output_result_dir, outcome_name, 'summary')\n",
    "\n",
    "    concept_id_name_dict = dict(zip(all_domain_df.concept_id, all_domain_df.concept_name))\n",
    "    concept_id_domain_dict = dict(zip(all_domain_df.concept_id, all_domain_df.concept_domain))\n",
    "\n",
    "    feature_selection_method_df_dict = {}\n",
    "\n",
    "    # In[ ] @ Feature_Selection 1 : statistics method\n",
    "    meas_concept_ids = list(set(all_domain_pivot_df.columns)&(set(meas_df.concept_id)))\n",
    "    drug_concept_ids = list(set(all_domain_pivot_df.columns)&(set(drug_df.concept_id)))\n",
    "    proc_concept_ids = list(set(all_domain_pivot_df.columns)&(set(proc_df.concept_id)))\n",
    "    cond_concept_ids = list(set(all_domain_pivot_df.columns)&(set(cond_df.concept_id)))\n",
    "\n",
    "    selected_features_with_t_test_df = getPairedTTest(all_domain_pivot_baseline_df, all_domain_pivot_df, meas_concept_ids)\n",
    "    selected_features_with_mcnemar_df = getMcnemarTest(all_domain_pivot_baseline_df, all_domain_pivot_df, drug_concept_ids + cond_concept_ids + proc_concept_ids)\n",
    "\n",
    "    selected_features_df = pd.concat([selected_features_with_t_test_df, selected_features_with_mcnemar_df], axis=0)\n",
    "    if not selected_features_df.empty:\n",
    "        selected_features_df.concept_id = selected_features_df.concept_id.astype(np.object)\n",
    "    selected_features_df = add_column_concept_name(selected_features_df, concept_id_name_dict)\n",
    "    selected_features_df = add_column_concept_domain(selected_features_df, concept_id_domain_dict)\n",
    "    write_file_method(selected_features_df, output_result_dir, outcome_name, 'statistics')\n",
    "    feature_selection_method_df_dict['statistics'] = selected_features_df\n",
    "\n",
    "    len(all_domain_df.concept_id.unique()), len(all_domain_baseline_df.concept_id.unique())\n",
    "\n",
    "    # In[ ] @ imputation missing data\n",
    "    all_domain_pivot_df = impute_conditional_data(all_domain_pivot_df, meas_concept_ids)\n",
    "    all_domain_pivot_df = impute_binary_data(all_domain_pivot_df, drug_concept_ids + proc_concept_ids + cond_concept_ids)\n",
    "\n",
    "    meas_concept_ids = list(set(all_domain_pivot_baseline_df.columns)&(set(meas_df.concept_id)))\n",
    "    drug_concept_ids = list(set(all_domain_pivot_baseline_df.columns)&(set(drug_df.concept_id)))\n",
    "    proc_concept_ids = list(set(all_domain_pivot_baseline_df.columns)&(set(proc_df.concept_id)))\n",
    "    cond_concept_ids = list(set(all_domain_pivot_baseline_df.columns)&(set(cond_df.concept_id)))\n",
    "\n",
    "    all_domain_pivot_baseline_df = impute_conditional_data(all_domain_pivot_baseline_df, meas_concept_ids)\n",
    "    all_domain_pivot_baseline_df = impute_binary_data(all_domain_pivot_baseline_df, drug_concept_ids + proc_concept_ids + cond_concept_ids)\n",
    "\n",
    "    X_total, y_total, cols = getxydata(all_domain_pivot_df)\n",
    "    X_total = normalization_minmax(X_total)\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_total, y_total, test_size=0.3, random_state=1, stratify=y_total) \n",
    "\n",
    "    # check for nan / infinite value \n",
    "    np.argwhere(np.isnan(X_train)), np.argwhere(np.isinf(X_train))\n",
    "\n",
    "    # In[ ] @ Feature_Selection 2 : VarianceThreshold\n",
    "    selector = VarianceThreshold(1e-3)\n",
    "    X_train_sel = selector.fit_transform(X_train)\n",
    "    X_test_sel = selector.transform(X_test)\n",
    "    print(X_train.shape, X_train_sel.shape)\n",
    "    selected_features = selector.get_feature_names_out(cols)\n",
    "    selected_features_df = make_concepts_df(selected_features, concept_id_name_dict, concept_id_domain_dict)\n",
    "    write_file_method(selected_features_df, output_result_dir, outcome_name, 'VT')\n",
    "    feature_selection_method_df_dict['VT'] = selected_features_df\n",
    "\n",
    "    # In[ ] @ Feature_Selection 3 : SelectPercentile\n",
    "    selector = SelectPercentile(chi2, percentile=3) # now select features based on top 10 percentile\n",
    "    X_train_sel = selector.fit_transform(X_train, y_train)\n",
    "    X_test_sel = selector.transform(X_test)\n",
    "    print(X_train.shape, X_train_sel.shape)\n",
    "    selected_features = selector.get_feature_names_out(cols)\n",
    "    selected_features_df = make_concepts_df(selected_features, concept_id_name_dict, concept_id_domain_dict)\n",
    "    write_file_method(selected_features_df, output_result_dir, outcome_name, 'percentile')\n",
    "    feature_selection_method_df_dict['percentile'] = selected_features_df\n",
    "\n",
    "    # In[ ] @ Feature_Selection 4 : SelectKBest\n",
    "    selector = SelectKBest(score_func=chi2, k=50)\n",
    "    X_train_sel = selector.fit_transform(X_train, y_train)\n",
    "    X_test_sel = selector.transform(X_test)\n",
    "    print(X_train.shape, X_train_sel.shape)\n",
    "    selected_features = selector.get_feature_names_out(cols)\n",
    "    selected_features_df = make_concepts_df(selected_features, concept_id_name_dict, concept_id_domain_dict)\n",
    "    write_file_method(selected_features_df, output_result_dir, outcome_name, 'KBest')\n",
    "    feature_selection_method_df_dict['KBest'] = selected_features_df\n",
    "\n",
    "    # In[ ] @ Feature_Selection 5 : ExtraTreesClassifier\n",
    "    treebasedclf = ExtraTreesClassifier(n_estimators=50)\n",
    "    treebasedclf = treebasedclf.fit(X_train, y_train)\n",
    "    model = SelectFromModel(treebasedclf, prefit=True)\n",
    "    X_train_sel = model.transform(X_train)\n",
    "    print(X_train.shape, X_train_sel.shape)\n",
    "    selected_features = model.get_feature_names_out(cols)\n",
    "    selected_features_df = make_concepts_df(selected_features, concept_id_name_dict, concept_id_domain_dict)\n",
    "    write_file_method(selected_features_df, output_result_dir, outcome_name, 'ExtraTrees')\n",
    "    feature_selection_method_df_dict['ExtraTrees'] = selected_features_df\n",
    "\n",
    "    # In[ ] @ Feature_Selection 6 : Lasso (1) > alpha = 0.1\n",
    "    lasso = Lasso(alpha=0.1)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    # print(lasso.coef_)\n",
    "    importance = np.abs(lasso.coef_)\n",
    "    selected_features = np.array(cols)[importance > 0]\n",
    "    selected_features_df = make_concepts_df(selected_features, concept_id_name_dict, concept_id_domain_dict)\n",
    "\n",
    "    write_file_method(selected_features_df, output_result_dir, outcome_name, 'lasso_0_1')\n",
    "    feature_selection_method_df_dict['lasso_0_1'] = selected_features_df\n",
    "\n",
    "    # In[ ] @ Feature_Selection 7 : Lasso (2) > alpha = 0.0.1\n",
    "    lasso = Lasso(alpha=0.01)\n",
    "    lasso.fit(X_train, y_train)\n",
    "    # print(lasso.coef_)\n",
    "    importance = np.abs(lasso.coef_)\n",
    "    selected_features = np.array(cols)[importance > 0]\n",
    "    selected_features_df = make_concepts_df(selected_features, concept_id_name_dict, concept_id_domain_dict)\n",
    "    write_file_method(selected_features_df, output_result_dir, outcome_name, 'lasso_0_0_1')\n",
    "    feature_selection_method_df_dict['lasso_0_0_1'] = selected_features_df\n",
    "\n",
    "    # In[ ] @ Feature_Selection 8 : mutual_info_classif\n",
    "    importances = mutual_info_classif(X_total, y_total, discrete_features='auto')\n",
    "    threshold = 0.001\n",
    "    selected_features = np.array(cols)[importance > threshold]\n",
    "    selected_features_df = make_concepts_df(selected_features, concept_id_name_dict, concept_id_domain_dict)\n",
    "    write_file_method(selected_features_df, output_result_dir, outcome_name, 'mutual')\n",
    "    feature_selection_method_df_dict['mutual'] = selected_features_df\n",
    "\n",
    "    # In[ ] @ all methods concatenate\n",
    "    concat_df = concat_all_methods(feature_selection_method_df_dict)\n",
    "    write_file_method(concat_df, output_result_dir, outcome_name, 'all_methods')\n",
    "    summary_concat_df = merge_summary_table_df(summary_df, concat_df)\n",
    "    write_file_method(summary_concat_df, output_result_dir, outcome_name, 'total')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for outcome_name in tqdm(cfg['drug'].keys()) :\n",
    "    try :\n",
    "        runTask(outcome_name)        \n",
    "    except :\n",
    "        traceback.print_exc()\n",
    "        log.error(traceback.format_exc())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9045736b69e2f8814b71be39877dcae222bacd6a95a19c9cc1b71f5c99b15c3e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
